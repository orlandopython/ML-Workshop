{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Scikit-Learn\n",
    "\n",
    "Introduces Scikit-Learn through two simple supervised learning algorithms.\n",
    "\n",
    "**Author**: Michael duPont\n",
    "\n",
    "Created for Orlando Python ML Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Welcome to the magical world of machine learning. Well...it's not really magic, but it can feel like it sometimes. Machine learning is, at its very core, pattern recognition, something we do ourselves very well. The field can be confusing sometimes with a sea of algorithms, but there are a few high-level categories that help us whittle down the initial list of potential models.\n",
    "\n",
    "![](http://datasentimentanalysis.com/wp-content/uploads/2015/01/ml_map1.png)\n",
    "\n",
    "This flow chart is provided by Scikit-Learn (which we'll shorten to sk-learn) which provides a quick road map to a potential algorithm. It doesn't include every algorithm available in sk-learn and likely won't be enough by itself, but it's good place to start when working with new problems.\n",
    "\n",
    "### Terminology\n",
    "\n",
    "Before we go further, let's define some terminology we'll be using.\n",
    "\n",
    "* **Model**: The algorithm which processes data and returns some prediction\n",
    "* **Feature**: An input to the model. Think column of a spreadsheet\n",
    "* **Target**: An output/desired result from a model\n",
    "* **Train**: Fitting a model to a given subset of the features and targets\n",
    "* **Test**: Test the accuracy of the trained model with the remainder of the features and targets\n",
    "* **Dimension**: The number of features used with a model\n",
    "* **Variability**: The (dis)simularity of data within a feature. Low variability ~ concentrated data\n",
    "\n",
    "### Categories\n",
    "\n",
    "There are three primary categories of machine learning algorithms:\n",
    "\n",
    "* **Supervised**: Used when the desired output of a model is known and can be trained on\n",
    "* **Unsupervised**: Used to deconstruct data and find unknown groupings\n",
    "* **Reinforcement**: Similar to supervised but driven towards maximizing reward with no set target\n",
    "\n",
    "\n",
    "### Subcategories\n",
    "\n",
    "The four groups highlighted in the flow chart are the most used subcategories in machine learning.\n",
    "\n",
    "* **Classification**: Supervised where the target value is binary or categorical\n",
    "* **Regression**: Supervised where the target value is in a continuous range of numbers\n",
    "* **Clustering**: Unsupervised used to identify and label groups of targets\n",
    "* **Dimensionality Reduction**: Unsupervised used to combine features while preserving variability\n",
    "\n",
    "For this intro, we're going to do a quick implementation of the two supervised subcategories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "We're going to start out with a binary classifier. We'll train a model to determine someone's sex based on their height.\n",
    "\n",
    "### Data\n",
    "\n",
    "By this point, I should have collected some sample data from the workshop attendees. Normally we'd split the dataset into training and testing data, but we're going to test on some people that we're included in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "opug = pd.read_csv(open('opug-heights.csv'), header=0, index_col=None)\n",
    "print('Height (in) and Sex (M/F) of OPUG attendees')\n",
    "display(opug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Because of the small data size, we'll use a [Support Vector Machine](http://scikit-learn.org/stable/modules/svm.html#classification) as our classifier. We'll use the same one later, but it will be configured as a regressor. I won't go too in-depth about the algorithm. For our purposes, know that it creates a boundary between our two target values and returns which side of the boundary new points are on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "#Reshape our data to a 2D array\n",
    "height = np.reshape(opug.height, (-1, 1))\n",
    "sex = list(opug.sex)\n",
    "print(\"Feature data (height)\\n\", height)\n",
    "print(\"\\nTarget data (sex)\\n\", sex)\n",
    "\n",
    "#Create and train our classifier\n",
    "clf = SVC()\n",
    "clf.fit(height, sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Now that our model is trained, let's have some fun with it. We'll use it to predict the sex of other attendees that weren't included in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clf.predict([[68],[64],[71]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Now we're going to work with a regression. I'm sure some people have created graphs using Excel, Sheets, Numbers, or any of the other spreadsheet applications. You usually have the option to include a \"line of best fit\" to help explain the data. In essance, this is a regression algorithm.\n",
    "\n",
    "![](Regression.png)\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "We're going to generate data that roughly follows a sin wave in the x-y plane. We're going to introduce some noise and see how well our model can infer the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Setting the seed / random_state allows us to tweak without the data changing\n",
    "np.random.seed(312)\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - np.random.rand(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our classification example, we have our full dataset already, so we'll need to split our existing data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=.20, random_state=42)\n",
    "print('Number of items in the training set:', X_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Again, we'll be using a [Support Vector Machine](http://scikit-learn.org/stable/modules/svm.html#regression) but as a regressor. We're going to show how the parameters of a model can affect its ability to represent the data. A requirement for SVMs is that it must make a linear boundary. However, we can use something called a kernel trick to shape the data to best fit the model. Our three kernels will be linear, rbf, and poly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "svr_lin = SVR(kernel='linear', C=1e3)\n",
    "svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "y_rbf = svr_rbf.fit(X_train, y_train).predict(X_test)\n",
    "y_lin = svr_lin.fit(X_train, y_train).predict(X_test)\n",
    "y_poly = svr_poly.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "In the chart earlier, the regression line is accompanied by an r^2 score. The closer this value is to one, the more accurately it represents the data. We'll use sk-learn's metrics module to compute this score for each of our kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('r^2 score for {}: {:.3f}'.format(svr_rbf.kernel, r2_score(y_test, y_rbf)))\n",
    "print('r^2 score for {}: {:.3f}'.format(svr_lin.kernel, r2_score(y_test, y_lin)))\n",
    "print('r^2 score for {}: {:.3f}'.format(svr_poly.kernel, r2_score(y_test, y_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our rbf score is really good, and they all make sense. Even if you don't know what rbf or poly do, it would make sense that the linear model scores the lowest for a sinusoidal dataset. Let's see why below.\n",
    "\n",
    "### Visulaization\n",
    "\n",
    "Finally, the fastest way to understand our data and models is to visualize them. We'll use pyplot from matplotlib to plot our training points, testing points, and three SVMs and visually compare them to the r^2 scores we just calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sort_pair(a, b):\n",
    "    \"\"\"Sorts two lists based on the values in the first list\"\"\"\n",
    "    srt = sorted(zip(a, b))\n",
    "    return zip(*srt)\n",
    "\n",
    "plt.scatter(X_train, y_train, c='k', label='train data')\n",
    "plt.scatter(X_test, y_test, c='c', label='test data')\n",
    "plt.hold('on')\n",
    "plt.plot(*sort_pair(X_test, y_rbf), c='g', label='RBF model')\n",
    "plt.plot(*sort_pair(X_test, y_lin), c='r', label='Linear model')\n",
    "plt.plot(*sort_pair(X_test, y_poly), c='b', label='Polynomial model')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of our eight testing points, one was a moderate outlier. However, while it dragged down a small part of the graph, the outlier didn't interfere with the rbf model's ability to infer the sinusoidal shape. The other two are not as well suited to this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
